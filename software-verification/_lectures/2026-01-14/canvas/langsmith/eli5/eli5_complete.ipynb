{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELI5 (Explain Like I'm 5) - Complete Tutorial\n",
    "\n",
    "This comprehensive notebook combines all aspects of building, testing, and evaluating an ELI5 application using:\n",
    "- **Ollama** (llama3.2:1b) for local LLM inference\n",
    "- **DuckDuckGo** for free web search (no API key required)\n",
    "- **LangGraph** for building the application\n",
    "- **LangSmith** for tracing, debugging, and evaluation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Tracing & Graph Building](#section-1)\n",
    "2. [Different Run Types](#section-2)\n",
    "3. [Debugging with LangGraph](#section-3)\n",
    "4. [Prompt Engineering](#section-4)\n",
    "5. [Experimentation & Evaluation](#section-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Tracing & Graph Building <a id=\"section-1\"></a>\n",
    "\n",
    "In this section, we'll build our ELI5 application using LangGraph and learn how to trace it with LangSmith."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Let's start by loading our environment variables from our .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "# Loads the following env variables\n",
    "# LANGSMITH_TRACING=true\n",
    "# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGSMITH_PROJECT=\"eli5-bot\"\n",
    "# LANGSMITH_API_KEY=\"<your_key>\"\n",
    "\n",
    "# No need for OPENAI_API_KEY or TAVILY_API_KEY anymore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Tools\n",
    "\n",
    "Let's set up DuckDuckGo search to allow our assistant to search the web when answering (free, no API key required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "web_search_tool = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
      "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
      "You have provided with relevant background context to answer the question.\n",
      "\n",
      "Question: {question} \n",
      "\n",
      "Context: {context}\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"You are a professor and expert in explaining complex topics in a way that is easy to understand. \n",
    "Your job is to answer the provided question so that even a 5 year old can understand it. \n",
    "You have provided with relevant background context to answer the question.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Application with Ollama\n",
    "\n",
    "**Important:** Make sure Ollama is running with `ollama serve` before executing the cells below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ollama Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    messages: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the question using DuckDuckGo.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "\n",
    "    # Web search using DuckDuckGo\n",
    "    try:\n",
    "        web_results = web_search_tool.invoke(question)\n",
    "        web_results = Document(page_content=web_results)\n",
    "        documents.append(web_results)\n",
    "    except Exception as e:\n",
    "        # Handle search errors gracefully\n",
    "        error_doc = Document(page_content=f\"Search error: {str(e)}\")\n",
    "        documents.append(error_doc)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "    \n",
    "def explain(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate response using Ollama\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "        \n",
    "    Returns:\n",
    "        state (dict): New key added to state, messages, that contains LLM generation\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    formatted = prompt.format(question=question, context=\"\\n\".join([d.page_content for d in documents]))\n",
    "    generation = llm.invoke([HumanMessage(content=formatted)])\n",
    "    return {\"question\": question, \"messages\": [generation]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "graph.add_node(\"explain\", explain)\n",
    "graph.add_node(\"search\", search)\n",
    "graph.add_edge(START, \"search\")\n",
    "graph.add_edge(\"search\", \"explain\")\n",
    "graph.add_edge(\"explain\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    for output in app.stream({\"question\": question}, stream_mode=\"updates\"):\n",
    "        if END in output or START in output:\n",
    "            continue\n",
    "        # Print any node outputs\n",
    "        for key, value in output.items():\n",
    "            if \"messages\" in value:\n",
    "                print(value[\"messages\"][0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there little friend! So, you want to know about something called \"complexity economics\"? Well, let me tell you all about it.\n",
      "\n",
      "You see, when we talk about money and how people make it, it's like a big puzzle. We have different pieces that fit together in certain ways, but sometimes they don't fit perfectly. That's where complexity comes in.\n",
      "\n",
      "Imagine you're playing with blocks, and each block represents something that makes up our economy. Some blocks are easy to understand, like \"people work\" or \"they buy things.\" But other blocks might be harder to grasp, like \"how people learn from mistakes\" or \"how the government helps people.\"\n",
      "\n",
      "Complexity economics is a way of thinking about how all these different pieces fit together. It's like trying to solve a big puzzle with many different parts that need to work well together.\n",
      "\n",
      "In this case, we're talking about how countries or regions make money and grow their economies. We want to understand why some places are rich and others are poor, even if it seems like it doesn't make sense at first.\n",
      "\n",
      "The good news is that complexity economics can help us answer these questions in a way that's more accurate than just looking at numbers and charts. It's like using a special tool that helps us see the whole picture, rather than just focusing on one part of the puzzle.\n",
      "\n",
      "Now, you might be wondering what this has to do with global markets. Well, it turns out that complex systems are all around us, even in things we don't think about every day. Like how the stock market works or how people interact with each other online.\n",
      "\n",
      "So, complexity economics is like a big umbrella that helps us understand all these different parts of our world and how they fit together. And who knows? Maybe one day you'll become an expert in this field and help make it even better!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is complexity economics?\"\n",
    "ask(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try More Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there little friend! Today, I want to tell you about something really cool called machine learning.\n",
      "\n",
      "You know how sometimes we play games or watch videos on our phones? And the computer helps us by making decisions based on what it sees in those videos or games? Like, if a video shows a cat playing with a ball, the computer might think \"oh, cats love balls!\" and show you more videos like that.\n",
      "\n",
      "Well, machine learning is kind of like that, but instead of just thinking about things, computers can learn from lots and lots of data. Data is like a big library of information, and machines can go through it and find patterns or connections between things.\n",
      "\n",
      "Imagine you have a toy box full of different toys, and each toy has a picture on it. The computer can look at all the pictures together and say \"oh, I see that most toys are red!\" or \"I see that most toys have wheels!\"\n",
      "\n",
      "That's basically what machine learning does. It helps computers learn from data and make predictions about new things based on what they've seen before.\n",
      "\n",
      "For example, if you want a computer to help you find the best park in your city, it can look at lots of pictures of parks and say \"oh, I see that most parks have swings!\" or \"I see that most parks have a big grassy area!\"\n",
      "\n",
      "So, machine learning is like having a super smart assistant that helps computers make decisions based on what they've learned from data. And the best part is, it can help us do lots of things without us even telling it how to do them!\n"
     ]
    }
   ],
   "source": [
    "ask(\"What is machine learning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask(\"How do airplanes fly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Different Run Types <a id=\"section-2\"></a>\n",
    "\n",
    "LangSmith supports many different types of Runs for better tracing and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Runs\n",
    "\n",
    "LangSmith supports these run types (specify in `@traceable` decorator):\n",
    "\n",
    "- **LLM**: Invokes an LLM\n",
    "- **Retriever**: Retrieves documents from databases or other sources\n",
    "- **Tool**: Executes actions with function calls\n",
    "- **Chain**: Default type; combines multiple Runs into a larger process\n",
    "- **Prompt**: Hydrates a prompt to be used with an LLM\n",
    "- **Parser**: Extracts structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Runs Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'message': {'role': 'assistant',\n",
       "    'content': 'Sure, what time would you like to book the table for?'}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like to book a table for two.\"},\n",
    "]\n",
    "\n",
    "output = {\n",
    "  \"choices\": [\n",
    "      {\n",
    "          \"message\": {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": \"Sure, what time would you like to book the table for?\"\n",
    "          }\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def chat_model(messages: list):\n",
    "  return output\n",
    "\n",
    "chat_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Runs Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'LangSmith Document 1',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar'}},\n",
       " {'page_content': 'LangSmith Document 2',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar'}},\n",
       " {'page_content': 'LangSmith Document 3',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar'}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _convert_docs(results):\n",
    "  return [\n",
    "      {\n",
    "          \"page_content\": r,\n",
    "          \"type\": \"Document\",\n",
    "          \"metadata\": {\"foo\": \"bar\"}\n",
    "      }\n",
    "      for r in results\n",
    "  ]\n",
    "\n",
    "@traceable(run_type=\"retriever\")\n",
    "def retrieve_langsmith_docs(query):\n",
    "  # Retriever returning hardcoded dummy documents.\n",
    "  contents = [\"LangSmith Document 1\", \"LangSmith Document 2\", \"LangSmith Document 3\"]\n",
    "  return _convert_docs(contents)\n",
    "\n",
    "retrieve_langsmith_docs(\"User query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"In New York City, you can expect a humid subtropical climate with cold winters and warm summers. Here's a breakdown of what you can typically expect:\\n\\n**Winter (December to February):**\\n\\n* Average high temperature: 38°F (3°C)\\n* Average low temperature: 24°F (-4°C)\\n* Snowfall: Moderate, with an average of 18 inches (45 cm) per year\\n* Expect occasional cold snaps and frosty mornings\\n\\n**Spring (March to May):**\\n\\n* Average high temperature: 58°F (14°C)\\n* Average low temperature: 43°F (6°C)\\n* Spring showers are common, with an average of 4-5 inches (10-13 cm) per month\\n* Expect occasional thunderstorms and pollen counts\\n\\n**Summer (June to August):**\\n\\n* Average high temperature: 84°F (29°C)\\n* Average low temperature: 64°F (18°C)\\n* Hot and humid, with an average of 3-4 days above 90°F (32°C) per month\\n* Expect occasional heatwaves and thunderstorms\\n\\n**Autumn (September to November):**\\n\\n* Average high temperature: 73°F (23°C)\\n* Average low temperature: 54°F (12°C)\\n* Comfortable temperatures, with an average of 3-4 days above 80°F (27°C) per month\\n* Expect occasional rain showers and cooler mornings\\n\\nKeep in mind that these are just averages, and actual weather conditions can vary from year to year. It's always a good idea to check the forecast before heading out or planning outdoor activities.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2026-01-07T22:01:24.6772836Z', 'done': True, 'done_reason': 'stop', 'total_duration': 39967442600, 'load_duration': 135939100, 'prompt_eval_count': 38, 'prompt_eval_duration': 601023700, 'eval_count': 330, 'eval_duration': 38341477000, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019b9a79-f5bd-7902-9656-078a85d0216d-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 38, 'output_tokens': 330, 'total_tokens': 368})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "ollama_client_tools = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "@traceable(run_type=\"tool\")\n",
    "def search_web(query: str):\n",
    "    \"\"\"Search the web for a query.\"\"\"\n",
    "    return f\"No result found for query: {query}\"\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_ollama(messages: List[dict], tools: Optional[List[dict]] = None) -> str:\n",
    "    \"\"\"Call Ollama LLM.\"\"\"\n",
    "    response = ollama_client_tools.invoke(messages)\n",
    "    return response\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def respond(inputs, tools):\n",
    "    \"\"\"Main response chain.\"\"\"\n",
    "    response = call_ollama(inputs, tools)\n",
    "    return response\n",
    "\n",
    "tools = [{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"search_web\",\n",
    "    \"description\": \"Search the web for a specific query\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\"query\": {\"type\": \"string\", \"description\": \"The query to search\"}},\n",
    "      \"required\": [\"query\"]\n",
    "    }\n",
    "  }\n",
    "}]\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "  {\"role\": \"user\", \"content\": \"What is the weather in NYC?\"},\n",
    "]\n",
    "\n",
    "respond(inputs, tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Debugging with LangGraph <a id=\"section-3\"></a>\n",
    "\n",
    "We've implemented 3 versions of our ELI5 Application using LangGraph (available in graphs.py):\n",
    "\n",
    "1. **Working version**: Functional implementation\n",
    "2. **Buggy version**: Does not explain concepts simply\n",
    "3. **Flaky version**: Has unreliable search\n",
    "\n",
    "All versions now use Ollama (llama3.2:1b) and DuckDuckGo search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangGraph Studio\n",
    "\n",
    "To debug and visualize these graphs, use LangGraph Studio:\n",
    "\n",
    "```bash\n",
    "langgraph dev\n",
    "```\n",
    "\n",
    "**Important:** Make sure Ollama is running with `ollama serve` before starting LangGraph dev.\n",
    "\n",
    "This will start a local server at `http://localhost:2024` where you can:\n",
    "- Visualize graph execution\n",
    "- Inspect intermediate states\n",
    "- Debug issues in real-time\n",
    "- Compare different versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Prompt Engineering <a id=\"section-4\"></a>\n",
    "\n",
    "Let's explore prompt engineering and management using LangSmith's PromptHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LangSmith Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Manage Prompts in PromptHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to pull existing prompt...\n",
      "✅ Successfully pulled existing prompt from LangSmith\n"
     ]
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langsmith.utils import LangSmithNotFoundError\n",
    "\n",
    "client = Client(api_key=LANGSMITH_API_KEY)\n",
    "\n",
    "# Define the prompt template\n",
    "eli5_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert at explaining complex topics in simple terms that a 5-year-old could understand. \n",
    "\n",
    "Your task is to take a complex question and context information, then provide a clear, simple explanation using:\n",
    "- Simple words and concepts\n",
    "- Analogies and examples from everyday life\n",
    "- Short sentences\n",
    "- Engaging and friendly tone\n",
    "\n",
    "Keep your explanation concise but complete.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Please explain this in simple terms that a 5-year-old would understand:\n",
    "\"\"\")])\n",
    "\n",
    "# Try to pull the prompt, if it doesn't exist, push it first\n",
    "try:\n",
    "    print(\"Trying to pull existing prompt...\")\n",
    "    prompt_from_hub = client.pull_prompt(\"eli5-concise\", include_model=True)\n",
    "    print(\"✅ Successfully pulled existing prompt from LangSmith\")\n",
    "except LangSmithNotFoundError:\n",
    "    print(\"❌ Prompt not found. Creating and pushing new prompt...\")\n",
    "    \n",
    "    # Push the prompt to LangSmith\n",
    "    client.push_prompt(\n",
    "        \"eli5-concise\",\n",
    "        object=eli5_prompt_template,\n",
    "        description=\"A prompt for explaining complex topics in simple terms\"\n",
    "    )\n",
    "    print(\"✅ Successfully pushed prompt to LangSmith\")\n",
    "    \n",
    "    # Now pull the prompt back\n",
    "    prompt_from_hub = client.pull_prompt(\"eli5-concise\", include_model=True)\n",
    "    print(\"✅ Successfully pulled the newly created prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PromptHub Prompt in Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "# Create Ollama client for prompting section\n",
    "ollama_client_prompting = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "@traceable\n",
    "def search_prompting(question):\n",
    "    try:\n",
    "        web_results = web_search_tool.invoke(question)\n",
    "        return web_results\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "    \n",
    "@traceable\n",
    "def explain_with_hub_prompt(question, context):\n",
    "    # Format the prompt with the question and context\n",
    "    messages = prompt_from_hub.format_messages(question=question, context=context)\n",
    "    \n",
    "    # Call Ollama with the formatted messages\n",
    "    response = ollama_client_prompting.invoke(messages)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "@traceable\n",
    "def eli5_with_hub_prompt(question):\n",
    "    context = search_prompting(question)\n",
    "    answer = explain_with_hub_prompt(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with PromptHub Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's an explanation of Complexity Economics that a 5-year-old can understand:\n",
      "\n",
      "Imagine you're playing with a big box of Legos. Each Lego block is like a tiny problem in the economy, and they all connect together to make something bigger.\n",
      "\n",
      "The Economic Complexity Index (ECI) is like a special tool that helps us see how well this whole system works. It's like measuring how many blocks you have in your box!\n",
      "\n",
      "But here's the cool thing: just because it's hard to predict what will happen next, doesn't mean we can't try. It's kind of like playing a game with Legos - sometimes things don't go as planned, but that's okay! We learn from our mistakes and make new rules.\n",
      "\n",
      "The problem is, most people think the economy is like a big, perfect puzzle. But it's actually more like a big box of blocks that are all connected in weird ways. That's why Complexity Economics helps us understand how these connections work together to create the economy we see today!\n"
     ]
    }
   ],
   "source": [
    "question = \"what is complexity economics?\"\n",
    "print(eli5_with_hub_prompt(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Experimentation & Evaluation <a id=\"section-5\"></a>\n",
    "\n",
    "Now let's run experiments and evaluate our application's performance on a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Application for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ollama client for experiments\n",
    "ollama_client_experiment = ChatOllama(\n",
    "    model=\"llama3.2:1b\",\n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "@traceable\n",
    "def search_experiment(question):\n",
    "    try:\n",
    "        web_results = web_search_tool.invoke(question)\n",
    "        return web_results\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {str(e)}\"\n",
    "    \n",
    "@traceable\n",
    "def explain_experiment(question, context):\n",
    "    formatted = prompt.format(question=question, context=context)\n",
    "    \n",
    "    response = ollama_client_experiment.invoke([\n",
    "        {\"role\": \"system\", \"content\": formatted},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ])\n",
    "    return response.content\n",
    "\n",
    "@traceable\n",
    "def eli5_experiment(question):\n",
    "    context = search_experiment(question)\n",
    "    answer = explain_experiment(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create/Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'eli5-golden' already exists with 9 examples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_name = \"eli5-golden\"\n",
    "\n",
    "# Read the dataset CSV file\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "# Convert to LangSmith format\n",
    "examples = []\n",
    "for _, row in df.iterrows():\n",
    "    examples.append({\n",
    "        \"inputs\": {\"question\": row[\"input_question\"]},\n",
    "        \"outputs\": {\"output\": row[\"output_output\"]}\n",
    "    })\n",
    "\n",
    "# Create dataset in LangSmith\n",
    "try:\n",
    "    existing_dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(f\"Dataset '{dataset_name}' already exists with {len(list(client.list_examples(dataset_name=dataset_name)))} examples\")\n",
    "except:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"ELI5 dataset for evaluating AI explanations\"\n",
    "    )\n",
    "    \n",
    "    client.create_examples(\n",
    "        inputs=[ex[\"inputs\"] for ex in examples],\n",
    "        outputs=[ex[\"outputs\"] for ex in examples],\n",
    "        dataset_id=dataset.id\n",
    "    )\n",
    "    \n",
    "    print(f\"Successfully created dataset '{dataset_name}' with {len(examples)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Code Evaluator (Conciseness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(outputs: dict) -> bool:\n",
    "    \"\"\"Check if output is <= 200 words.\"\"\"\n",
    "    words = outputs[\"output\"].split(\" \")\n",
    "    return len(words) <= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM-as-a-Judge Evaluator (Correctness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class CorrectnessScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer.\"\"\"\n",
    "    score: int = Field(description=\"Score from 0 to 1\")\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    eval_prompt = \"\"\"\n",
    "    You are an expert data labeler evaluating model outputs for correctness.\n",
    "\n",
    "    <Rubric>\n",
    "        A correct answer:\n",
    "        - Provides accurate information\n",
    "        - Uses suitable analogies and examples\n",
    "        - Contains no factual errors\n",
    "        - Is logically consistent\n",
    "    </Rubric>\n",
    "\n",
    "    <input>{}</input>\n",
    "    <output>{}</output>\n",
    "    <reference_outputs>{}</reference_outputs>\n",
    "    \"\"\".format(inputs[\"question\"], outputs[\"output\"], reference_outputs[\"output\"])\n",
    "    \n",
    "    structured_llm = ChatOllama(\n",
    "        model=\"llama3.2:1b\", # You can use a relatively large model like llama3.2:3b as evaluator here\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        temperature=0\n",
    "    ).with_structured_output(CorrectnessScore)\n",
    "    \n",
    "    generation = structured_llm.invoke([HumanMessage(content=eval_prompt)])\n",
    "    return generation.score == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Run Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(inputs: dict):\n",
    "    \"\"\"Run the application and return output in expected format.\"\"\"\n",
    "    try:\n",
    "        result = eli5_experiment(inputs[\"question\"])\n",
    "        return {\"output\": result if result else \"\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in run: {e}\")\n",
    "        return {\"output\": f\"Error: {str(e)}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment\n",
    "\n",
    "**Note:** This may take a while depending on your hardware and the number of examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'eli5-ollama-llama3.2-1b-d3dda18f' at:\n",
      "https://smith.langchain.com/o/4f5768b4-9e19-4abd-8067-61a12e1df8a4/datasets/cb04c0da-21f5-4d9b-969e-3d327d9e23c2/compare?selectedSessions=a2195c8c-ebec-4f03-ab1a-45a1ba154ab6\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caa884b17c0418ba67411cb790f117b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Experiment Complete!\n",
      "==================================================\n",
      "Results: <ExperimentResults eli5-ollama-llama3.2-1b-d3dda18f>\n"
     ]
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "results = evaluate(\n",
    "    run,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, conciseness],\n",
    "    experiment_prefix=\"eli5-ollama-llama3.2-1b\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Experiment Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Results: {results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "In this comprehensive notebook, we covered:\n",
    "\n",
    "1. **Tracing & Graph Building**: Built an ELI5 application with LangGraph and Ollama\n",
    "2. **Different Run Types**: Learned about LLM, Retriever, Tool, and Chain run types\n",
    "3. **Debugging**: Explored how to use LangGraph Studio for debugging\n",
    "4. **Prompt Engineering**: Managed prompts using LangSmith's PromptHub\n",
    "5. **Experimentation**: Evaluated our application using custom evaluators and LLM-as-a-judge\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Local Development**: Using Ollama (llama3.2:1b) allows for free, local LLM inference\n",
    "- **Free Search**: DuckDuckGo provides free web search without requiring an API key\n",
    "- **LangSmith Integration**: Tracing, debugging, and evaluation are seamlessly integrated\n",
    "- **Production Ready**: This pattern can be scaled to production with larger models and better infrastructure\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Experiment with larger Ollama models (llama3.2:3b, llama3.1:8b) for better quality\n",
    "2. Customize prompts in PromptHub for your specific use case\n",
    "3. Add more evaluators to measure different aspects of quality\n",
    "4. Deploy the application using LangGraph Cloud for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
